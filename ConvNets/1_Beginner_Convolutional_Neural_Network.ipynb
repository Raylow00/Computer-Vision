{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Beginner Convolutional Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpLfRMZ8kQT1"
      },
      "source": [
        "This is a notebook explaining how to construct a basic Convolutional Neural Network that is able to classify images and accomplish much more tasks besides image classification. More complex networks are modelled after this, hence feel free to play around with all published architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnDqeMpjZz2Q"
      },
      "source": [
        "To start off, we import layers and models from keras, a high level API that provideds an interface for Tensorflow to utilize the underlying artificial neural networks in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q72xG0kH77pR"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWADvl7GY7eN"
      },
      "source": [
        "How a Convolutional Neural Network is constructed?\n",
        "\n",
        "- Convolutional layer\n",
        "- Non-linearity\n",
        "- Pooling / Downsampling\n",
        "- Fully connected layer\n",
        "\n",
        "\n",
        "** Read the end of this notebook to see how we can tweak these parameters in a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS3JTBk1ahRv"
      },
      "source": [
        "The Sequential() constructor creates a plain stack of layers with only one input tensor and one output tensor - say, we input one image and expect a category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF_cMmC18wgE"
      },
      "source": [
        "model = models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUkpT11bbyjJ"
      },
      "source": [
        "We can add a layer of convolution to start learning the features. Here, passing in 32 shows that we are learning 32 features, each with a kernel size of (3, 3). The activation function chosen here is 'relu', which stands for Rectified Linear Unit. ReLU is chosen as it yields better results than other activation functions like sigmoid or hyperbolic tangent. Finally, we pass in the input shape of our image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBt-enWsbtEQ"
      },
      "source": [
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVxj3x4Ucg4w"
      },
      "source": [
        "Usually, after each convolutional layer, we add a MaxPooling2D layer to downsample the image in order to preserve spatial data and at the same time, generalize the learning behaviour. This takes in a window size of(2, 2) here, where the maximum value in each window is taken."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF5jgp93buzH"
      },
      "source": [
        "model.add(layers.MaxPooling2D(2, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjnlrkrAbw_Y"
      },
      "source": [
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzAtgYNpdJCX"
      },
      "source": [
        "We can view the whole model using summary(). The summary gives us the number of parameters that it needs to train with every layer, and the shape of the output tensor after each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QirKaxWc-hjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c40798-a4fc-4436-b5fd-df862d2b6a8d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxj4WfIidYg5"
      },
      "source": [
        "Finally, we would flatten the layers to serialize a multidimensional tensor, meaning the tensor is reshaped to have the same shape that is equal to the number of elements in a tensor\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0_l5d9dedCC"
      },
      "source": [
        "Before we are able to finish up with the model, we add a Dense layer so that we can connect all the layers together, which we call it a densely/fully connected layer. This layer is just a regular layer of neurons taking in as input the neurons in the previous layer and has an activation function *(activation(dot(input, kernel) + bias)* to produce an output tensor of the shape defined in the first argument - basically *(batch_size, units)*, here 10 and 64 units. The last layer can be seen as an output tensor of shape *(None, 10)* so that we can detect 10 different classes of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdXwVXg9-pku"
      },
      "source": [
        "# Flatten\n",
        "# Fully connected layer\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLIz5csc-5Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9dc8338-ed15-4137-a689-61f73b7f53c9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDIZyQIpgZrf"
      },
      "source": [
        "Now that we have the model setup, we are moving to feeding the model input data to train it. By using the MNIST data which is a dataset comprising all 10 digits handwritten which had been correctly identified and labelled, we prepare the data before feeding it to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL3IVrVu-6YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841ce85a-4a05-40d8-916d-afcbf851c359"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzqZG7Rsgt3m"
      },
      "source": [
        "Clearly, we can see that the datatset has 70,000 images of handwritten digits, with 60,000 in our training set and the remaining 10,000 in our test set. All of which having a size of 28 pixels by 28 pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnNBC9VJ_H6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad4961c-5b0b-4bd5-b2c6-ab0a50fcc6a3"
      },
      "source": [
        "print(train_images.shape)\n",
        "print(test_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mdyJw08g8JX"
      },
      "source": [
        "One crucial step is to expand the dimensions of our images. The reason for this is because our convolutional neural network takes in as input matrices of size *(batch_size, dim1, dim2, input_channel)*. Hence we are merely adding the extra dimension with essentially the same data as our input.\n",
        "\n",
        "After which, we normalize our images. This is another crucial step for data normalization to ensure that the data distribution for each input parameter(here called pixels) is similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd1lEVlU_Tjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4383d4-dc27-46cf-a643-4a18e0ab2c0e"
      },
      "source": [
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "print(train_images.shape)\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "print(test_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36fL3K0c_nsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b37568b5-071f-4f1a-d59e-e0e9a152b2c7"
      },
      "source": [
        "print(train_images[0].shape)\n",
        "print(test_images[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n",
            "(28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0lGOTBAilxE"
      },
      "source": [
        "Because our images can be classified into 10 different categories, and that the categories are distinct, we need to convert the integers to binary class matrix, in other words a vector with shape that the model can output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbkSA3TP_tZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ffc47e-dcb9-4ef6-9679-96bedca9adf3"
      },
      "source": [
        "print(train_labels[0])\n",
        "print(test_labels[0])\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "print(train_labels[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "7\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muSb8vqBjM1W"
      },
      "source": [
        "One last step before we start training the model by feeding training images is to compile the model. This is to tell the model what kind of loss function and optimzer we are using. The loss function here is categorical_crossentropy, as this is a classification of multi-category case. While the optimizer used here is RMSprop, a gradient descent algorithm proposed by Geoff Hinton that helps the model learn by a defined learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQ55gFB_863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb787dce-a179-4767-8783-995daed49bb4"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 49s 52ms/step - loss: 0.1736 - accuracy: 0.9460\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 48s 51ms/step - loss: 0.0473 - accuracy: 0.9852\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 48s 51ms/step - loss: 0.0327 - accuracy: 0.9896\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 48s 51ms/step - loss: 0.0243 - accuracy: 0.9926\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 49s 52ms/step - loss: 0.0205 - accuracy: 0.9936\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 49s 52ms/step - loss: 0.0159 - accuracy: 0.9953\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 48s 51ms/step - loss: 0.0129 - accuracy: 0.9962\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 48s 51ms/step - loss: 0.0105 - accuracy: 0.9966\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 48s 52ms/step - loss: 0.0088 - accuracy: 0.9972\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 49s 52ms/step - loss: 0.0086 - accuracy: 0.9971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f81bd2a47b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuGN123skDWh"
      },
      "source": [
        "Finally, a simple evaluation can tell how accurate our model is, together with the loss. However, much more work needs to be done if the use case of the CNN model is different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TauQXA_yARVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c60fc0d-ff7d-42a5-d187-47d379cbb294"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0449 - accuracy: 0.9902\n",
            "0.04492592439055443\n",
            "0.9901999831199646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK_FLMoVg9Z9"
      },
      "source": [
        "# Parameters to Keras Conv2D Class\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8EyqcuAhbt4"
      },
      "source": [
        "**Filters**\n",
        "\n",
        "- The number of filters the convolutional layer will learn\n",
        "\n",
        "The layers early in the network will learn fewer filters compared to the layers deeper in the network, hence the larger the network, the more filters the network has to learn.\n",
        "\n",
        "- The number of filters *usually* go with powers of 2, starting with 32, then go up to 64 and 128 on the fewer side, and it can go up to 1024 for larger networks. Keep in mind that the more filters your network has to learn, the more expensive your network is.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzg4xh0piz-n"
      },
      "source": [
        "**Kernel size**\n",
        "\n",
        "- The size of the 2D convolution window, usually width and height of odd numbers\n",
        "\n",
        "This is a tuple specifying the width and height of the convolution window where our filters are learned.\n",
        "\n",
        "- This is usually decided upon the size of your images. If the image size is larger than 128 x 128, the filter size can be 5x5 or 7x7 in order to learn the features more quickly, and then for deeper layers, a filter size of 3x3 is usually sufficient. \n",
        "- It might be a good idea to stick with kernel sizes of 1x1 and 3x3 if the image size is smaller than 128 x 128, in order to learn the necessary features.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9wPENsqj7dQ"
      },
      "source": [
        "**Strides**\n",
        "\n",
        "- tuple of 2 integers, specifying the step of the convolution window along the x and y axis.\n",
        "\n",
        "The stride here is how we move our kernel size along the x and y axis of our input image when learning filters. In other words, we are scanning the convolution window of kernel size along the x and y axis of the input image. The stride refers to how many units to the left/right and up/down we shift the window.\n",
        "\n",
        "- Usually, the stride here replaces the use of maxpooling, which is a method to reduce spatial dimensions for generalization.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VmgWTKqk2S8"
      },
      "source": [
        "**Padding**\n",
        "\n",
        "- valid or same\n",
        "\n",
        "Valid:\n",
        "\n",
        "The input is not zero-padded and the spatial dimensions are allowed to be reduced through the convolution operation.\n",
        "\n",
        "Same:\n",
        "\n",
        "The spatial dimensions of the input are preserved such that the output volume size matches the input volume size.\n",
        "\n",
        "\n",
        "You could set this flag to same, and then reduce the spatial dimension either by applying maxpooling or striding.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANNwRtsoHT4"
      },
      "source": [
        "**Activation**\n",
        "\n",
        "- The activation function to perform after the convolution operation.\n",
        "- Usually 'relu', 'softmax' or 'sigmoid'.\n",
        "- Best to use this parameter as this helps keep your code clean with the activation function specified within the convolutional layer itself.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqWLEzdlpAb-"
      },
      "source": [
        "**use_bias**\n",
        "\n",
        "- whether a bias vector is added to the convolutional layer\n",
        "- Best to use this parameter as adding a bias will usually add a little more flexibility to the model.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iMcyelG0aw6"
      },
      "source": [
        "*These are just the few of the many parameters in the Conv2D class and you can check out the Keras documentation to learn more about each parameter and how each one would affect the model, because ultimately, it is these parameters we are tweaking that will bring the best results from the model.*\n"
      ]
    }
  ]
}